{
    "steps": [
        {
            "step_type": "simple_multiple_choice",
            "step_name": "ARC Challenge MCP",
            "dataset_config": {
                "type": "arc_challenge",
                "dataset_kwargs": {
                    "seed": 2,
                    "split": "test",
                    "name_or_path": "allenai/ai2_arc",
                    "config_name": "ARC-Challenge",
                    "fewshot_split": "train",
                    "fewshot_num": 25,
                    "tokenizer_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                    "multiple_choice_template_name": "default",
                    "system_prompt": "You are a helpful assistant that answers multiple choice problems correctly. You strictly follow the output format by outputing only the answer key without any explanation."
                }
            },
            "inference_config": {
                "type": "local_hf",
                "output_path": "./dev/outputs",
                "inference_kwargs": {
                    "model_path": "meta-llama/Llama-2-7b-chat-hf",
                    "generation_config": {
                        "stop_sequences": ["A", "B", "C", "D", "E"]
                    },
                    "device": "cuda",
                    "num_gpus_per_model": 1,
                    "num_gpus_total": 4,
                    "max_gpu_memory": null,
                    "trial_run": true,
                    "dump_individual_rsp": true
                }
            },
            "eval_config": {
                "aggregate_mode": "ignore_augmented"
            }
        }
    ]
}
